{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e2e8ca4",
   "metadata": {
    "tags": [
     "fr"
    ]
   },
   "source": [
    "# Airflow\n",
    "\n",
    "## Qu'est-ce qu'Airflow ?\n",
    "\n",
    "Apache Airflow est une plateforme open-source conçue pour créer, planifier et surveiller des workflows de manière programmatique.\n",
    "\n",
    "Elle est largement utilisée pour orchestrer des pipelines de données complexes et automatiser des tâches en ingénierie des données, en apprentissage automatique et dans d'autres domaines.\n",
    "\n",
    "Airflow permet aux utilisateurs de définir des workflows sous forme de graphes acycliques dirigés (DAG), où chaque tâche représente une étape du pipeline. Ces workflows peuvent être planifiés pour s'exécuter à des intervalles spécifiques et surveillés pour leur statut d'exécution.\n",
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "De nombreuses façons d'installer Airflow (notamment avec PyPi).\n",
    "\n",
    "https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html\n",
    "\n",
    "Pour le lancer via Docker c'est ici :\n",
    "\n",
    "https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html\n",
    "\n",
    "Notes :\n",
    "\n",
    "- Vous pouvez l'installer avec PyPI\n",
    "- Ensuite, vous pouvez le lancer en mode autonome. Dans ce cas, utilisez cette ligne de commande pour modifier la verbosité :\n",
    "\n",
    "```shell\n",
    "export AIRFLOW__LOGGING__LOGGING_LEVEL=WARNING\n",
    "airflow standalone\n",
    "```\n",
    "\n",
    "## Concepts de base d'Airflow\n",
    "\n",
    "Vidéo de coder2j :\n",
    "\n",
    "https://youtu.be/K9AnJ9_ZAXE?si=FMot2dGl5L26u-cT&t=1078\n",
    "## DAGs et Workflows\n",
    "\n",
    "**Workflow :**  \n",
    "Un workflow est une séquence de tâches ou de processus exécutés pour atteindre un objectif spécifique. Dans le contexte d'Airflow, un workflow est représenté comme un DAG, où chaque nœud est une tâche et les arêtes (appelées aussi \"lignes\") définissent les dépendances. Les workflows automatisent et orchestrent des processus complexes, tels que les pipelines de données, en gérant l'exécution, la planification et la surveillance des tâches.\n",
    "\n",
    "**DAG (Directed Acyclic Graph) :**  \n",
    "Un DAG (en français : Graphe Orienté Acyclique) est un ensemble de toutes les tâches que vous souhaitez exécuter, organisé de manière à montrer clairement leurs relations et dépendances.\n",
    "\n",
    "Dans Airflow, un DAG définit la structure de votre workflow, garantissant que chaque tâche est exécutée dans le bon ordre et seulement lorsque toutes ses dépendances sont satisfaites. La propriété \"acyclique\" signifie qu'il n'y a pas de tâches effectuant des boucles.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"files/airflow_dag_tasks_and_operators.png\" alt=\"Airflow DAG, Tâches et Opérateurs\" style=\"max-width: 75%;\" source=\"https://www.youtube.com/watch?v=K9AnJ9_ZAXE\">\n",
    "</p>\n",
    "\n",
    "**Date d'exécution :**  \n",
    "La date d'exécution est un horodatage logique qui représente le moment où une exécution de DAG est planifiée pour démarrer. Elle est utilisée par Airflow pour suivre et identifier les exécutions d'un workflow (et donc pas nécessairement le moment réel de l'exécution).\n",
    "\n",
    "**Task Instance :**  \n",
    "Une instance de tâche ou *Task Instance* est une exécution spécifique d'une tâche pour une exécution de DAG et une date d'exécution données. Elle représente l'état et le résultat d'une seule exécution d'une tâche au sein d'un workflow. Comme variable, elle est souvent nommée `ti`.\n",
    "\n",
    "**Exécution de DAG (DAG Run) :**  \n",
    "Une exécution de DAG est une instance d'exécution d'un DAG, déclenchée soit par un planning, soit manuellement. Chaque exécution de DAG est associée à une date d'exécution et contient toutes les instances de tâches pour cette exécution particulière.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"files/airflow_dag_run_execution_date_run_task_instance.png\" alt=\"Airflow DAG, Date d'exécution et Instance de tâche\" style=\"max-width: 75%;\">\n",
    "</p>\n",
    "\n",
    "**airflow.cfg :**  \n",
    "`airflow.cfg` est le fichier de configuration principal d'Apache Airflow. Il contient les paramètres qui contrôlent le comportement des composants d'Airflow, tels que le serveur web, le scheduler (planificateur), l'exécuteur et les connexions à la base de données. Vous pouvez modifier ce fichier pour personnaliser votre environnement Airflow.\n",
    "\n",
    "**Serveur Web :**  \n",
    "Le serveur web fournit une interface utilisateur pour Airflow, permettant aux utilisateurs de surveiller les DAGs, de déclencher des exécutions, de consulter les journaux et de gérer les workflows. Il est généralement accessible via un navigateur et est essentiel pour la gestion et le dépannage des workflows.\n",
    "\n",
    "**Scheduler:**  \n",
    "Le scheduler (planificateur) est responsable de la surveillance des définitions de DAG et de la planification des instances de tâches pour exécution en fonction de leurs dépendances et de leurs plannings. Il détermine quand chaque tâche doit s'exécuter et les envoie à l'exécuteur.\n",
    "\n",
    "**Exécuteur (Local ou Séquentiel) :**  \n",
    "L'exécuteur détermine comment et où les tâches sont exécutées.  \n",
    "- **Exécuteur local :** Exécute les tâches dans des processus parallèles (la plupart du temps) sur la même machine, adapté aux charges de travail petites à moyennes.\n",
    "- **Exécuteur séquentiel :** Exécute une tâche à la fois, principalement utilisé pour les tests ou le développement.\n",
    "\n",
    "**Worker :**  \n",
    "Un worker est un processus qui exécute les tâches définies dans vos DAGs. Dans des configurations distribuées (comme avec l'exécuteur Celery), plusieurs workers peuvent exécuter des tâches en parallèle sur différentes machines, augmentant ainsi l'évolutivité et la fiabilité. Chaque worker reçoit des ressources de l'exécuteur pour effectuer ses tâches.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"files/airflow_basic_architecture.png\" alt=\"Architecture de base d'Airflow\" style=\"max-width: 75%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a6971e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Airflow\n",
    "\n",
    "## What is Airflow?\n",
    "\n",
    "Apache Airflow is an open-source platform designed to programmatically author, schedule, and monitor workflows.\n",
    "\n",
    "It is widely used for orchestrating complex data pipelines and automating tasks in data engineering, machine learning, and other domains.\n",
    "\n",
    "Airflow allows users to define workflows as Directed Acyclic Graphs (DAGs), where each task represents a step in the pipeline. These workflows can be scheduled to run at specific intervals and monitored for execution status.\n",
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "Plenty of different ways to install Airflow :\n",
    "\n",
    "https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html\n",
    "\n",
    "Notes:\n",
    "\n",
    "- You can install it with PyPI\n",
    "- Then you can launch it on standalone mode. in that case use this command line to modify the verbosity:\n",
    "\n",
    "```shell\n",
    "export AIRFLOW__LOGGING__LOGGING_LEVEL=WARNING\n",
    "airflow standalone\n",
    "```\n",
    "\n",
    "## Airflow Core concepts\n",
    "\n",
    "Video from coder2j:\n",
    "\n",
    "https://youtu.be/K9AnJ9_ZAXE?si=FMot2dGl5L26u-cT&t=1078\n",
    "## DAGs and Workflows\n",
    "\n",
    "**Workflow:**  \n",
    "A workflow is a sequence of tasks or processes that are executed to achieve a specific goal. In the context of Airflow, a workflow is represented as a DAG, where each node is a task and edges define dependencies. Workflows automate and orchestrate complex processes, such as data pipelines, by managing the execution, scheduling, and monitoring of tasks.\n",
    "\n",
    "**DAG (Directed Acyclic Graph):**\n",
    "A DAG is a collection of all the tasks you want to run, organized in a way that clearly shows their relationships and dependencies.\n",
    "\n",
    "In Airflow, a DAG defines the structure of your workflow, ensuring that each task is executed in the correct order and only once all its dependencies have been met. The \"acyclic\" property means there are no loops-tasks.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"files/airflow_dag_tasks_and_operators.png\" alt=\"Airflow DAG, Tasks, and Operators\" style=\"max-width: 75%;\" source=\"https://www.youtube.com/watch?v=K9AnJ9_ZAXE\">\n",
    "</p>\n",
    "\n",
    "**Execution Date:**  \n",
    "The execution date is a logical timestamp that represents when a DAG run is scheduled to start. It is used by Airflow to track and identify runs of a workflow, not necessarily the actual time the workflow is executed.\n",
    "\n",
    "**Task Instance:**  \n",
    "A task instance is a specific run of a task for a given DAG run and execution date. It represents the state and result of a single execution of a task within a workflow. As a variable it is often named `ti`.\n",
    "\n",
    "**DAG Run:**  \n",
    "A DAG run is an instance of a DAG execution, triggered either by a schedule or manually. Each DAG run is associated with an execution date and contains all the task instances for that particular run.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"files/airflow_dag_run_execution_date_run_task_instance.png\" alt=\"Airflow DAG, Run Execution Date and Task Instance\" style=\"max-width: 75%;\">\n",
    "</p>\n",
    "\n",
    "**airflow.cfg:**  \n",
    "`airflow.cfg` is the main configuration file for Apache Airflow. It contains settings that control the behavior of Airflow components, such as the web server, scheduler, executor, and database connections. You can modify this file to customize your Airflow environment.\n",
    "\n",
    "**Web Server:**  \n",
    "The web server provides a user interface for Airflow, allowing users to monitor DAGs, trigger runs, view logs, and manage workflows. It is typically accessible via a browser and is essential for workflow management and troubleshooting.\n",
    "\n",
    "**Scheduler:**  \n",
    "The scheduler is responsible for monitoring DAG definitions and scheduling task instances for execution based on their dependencies and schedules. It determines when each task should run and sends them to the executor.\n",
    "\n",
    "**Executor (Local or Sequential):**  \n",
    "The executor determines how and where tasks are executed.  \n",
    "- **Local Executor:** Runs tasks in parallel processes (most of the time) on the same machine, suitable for small to medium workloads. \n",
    "- **Sequential Executor:** Runs one task at a time, mainly used for testing or development.\n",
    "\n",
    "**Worker:**  \n",
    "A worker is a process that executes the actual tasks defined in your DAGs. In distributed setups (like with the Celery executor), multiple workers can run tasks in parallel across different machines, increasing scalability and reliability. Each worker receives resources from the executor to perform its tasks.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"files/airflow_basic_architecture.png\" alt=\"Airflow Basic Architecture\" style=\"max-width: 75%;\">\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc25f4c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
